

# Session2- Part1

Backpropagation for Multi Layer Perceptron

Let’s look at the step by step building methodology of Neural Network (MLP with one hidden layer, similar to above-shown architecture). At the output layer, we have only one neuron as we are solving a binary classification problem (predict 0 or 1). 

 

![1539020692640](C:\Users\u6yuv\AppData\Roaming\Typora\typora-user-images\1539020692640.png)



**Step-0**

 <u>Read input and output</u>

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_la |      |      | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | -------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ---- |
| 1    | 0    | 1    | 1    |      |      |      |      |      |      |                    |      |      |                      |      |      |      |      |        | 0    |      |
| 1    | 0    | 0    | 1    |      |      |      |      |      |      |                    |      |      |                      |      |      |      |      |        | 1    |      |
| 1    | 1    | 0    | 1    |      |      |      |      |      |      |                    |      |      |                      |      |      |      |      |        | 0    |      |
|      |      |      |      |      |      |      |      |      |      |                    |      |      |                      |      |      |      |      |        |      |      |



**Step1**

<u>Initialize weights and biases with random values</u>

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_la |      |      | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | -------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ---- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 |                    |      |      |                      |      |      | 0.45 | 0.5  |        | 0    |      |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      |                    |      |      |                      |      |      | 0.43 |      |        | 1    |      |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      |                    |      |      |                      |      |      | 0.12 |      |        | 0    |      |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                      |      |      |      |      |        |      |      |



**Step 2**

<u>Calculate hidden layer input</u> 

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_la |      |      | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | -------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ---- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 |                      |      |      | 0.45 | 0.5  |        | 0    |      |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 |                      |      |      | 0.43 |      |        | 1    |      |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 |                      |      |      | 0.12 |      |        | 0    |      |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                      |      |      |      |      |        |      |      |

**Step-3**

 <u>Perform non-linear transformation on hidden linear input</u>

*hidden_layer_activations = sigmoid(hidden_layer_input)*



| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ---- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.45 | 0.5  |        | 0    |      |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.43 |      |        | 1    |      |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.12 |      |        | 0    |      |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |      |



**Step-4**

<u>Perform linear and non-linear transformation of hidden layer activation at output layer</u>

*output_layer_input = matrix_dot_product (hidden_layer_activations * wout ) + bout*
*output = sigmoid(output_layer_input)*

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ---- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.45 | 0.5  | 0.79   | 0    |      |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.43 |      | 0.79   | 1    |      |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.12 |      | 0.80   | 0    |      |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |      |

**Step5**

<u>Calculate gradient of Error(E) at output layer</u>
​	*E = y-output*

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E     |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ----- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.45 | 0.5  | 0.79   | 0    | -0.79 |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.43 |      | 0.79   | 1    | 0.21  |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.12 |      | 0.80   | 0    | -0.80 |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |       |

**Step6**

<u>Compute slope at output and hidden layer</u>
*Slope_output_layer= derivatives_sigmoid(output)*
*Slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)*																				

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E     |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ----- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.45 | 0.5  | 0.79   | 0    | -0.79 |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.43 |      | 0.79   | 1    | 0.21  |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.12 |      | 0.80   | 0    | -0.80 |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |       |

| Slope of hidden layer |      |      |
| :-------------------- | ---- | ---- |
| 0.14                  | 0.12 | 0.14 |
| 0.15                  | 0.13 | 0.20 |
| 0.07                  | 0.08 | 0.15 |

| Slope of output layer |
| :-------------------: |
|         0.16          |
|         0.17          |
|         0.16          |

​																																			Step 7 Compute delta at output layer

d_output = E * slope_output_layer

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E     |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ----- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.45 | 0.5  | 0.79   | 0    | -0.79 |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.43 |      | 0.79   | 1    | 0.21  |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.12 |      | 0.80   | 0    | -0.80 |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |       |

| Slope of hidden layer |      |      |
| --------------------- | ---- | ---- |
| 0.14                  | 0.12 | 0.14 |
| 0.15                  | 0.13 | 0.20 |
| 0.07                  | 0.08 | 0.15 |

| Slope of output |
| --------------- |
| 0.16            |
| 0.17            |
| 0.16            |

| delta output |
| ------------ |
| -0.13        |
| 0.04         |
| -0.13        |

**Step-8**

 <u>Calculate Error at hidden layer</u>

*Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)*

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E     |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ----- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.45 | 0.5  | 0.79   | 0    | -0.79 |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.43 |      | 0.79   | 1    | 0.21  |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.12 |      | 0.80   | 0    | -0.80 |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |       |

| Slope of hidden layer |      |      |
| --------------------- | ---- | ---- |
| 0.14                  | 0.12 | 0.14 |
| 0.15                  | 0.13 | 0.20 |
| 0.07                  | 0.08 | 0.15 |

| delta output |
| ------------ |
| -0.13        |
| 0.04         |
| -0.13        |

| Slope of output |
| --------------- |
| 0.16            |
| 0.17            |
| 0.16            |

| Error at hidden layer |        |        |
| --------------------- | ------ | ------ |
| -0.059                | -0.059 | -0.059 |
| 0.015                 | 0.015  | 0.015  |
| -0.015                | -0.015 | -0.015 |



**Step 9**

<u>Compute delta at hidden layer</u>
*d_hidden_layer = (Error_at_hidden_layer * slope_hidden_layer)*

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E     |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ----- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.45 | 0.5  | 0.79   | 0    | -0.79 |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.43 |      | 0.79   | 1    | 0.21  |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.12 |      | 0.80   | 0    | -0.80 |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |       |

| Slope of hidden layer |      |      |
| --------------------- | ---- | ---- |
| 0.14                  | 0.12 | 0.14 |
| 0.15                  | 0.13 | 0.20 |
| 0.07                  | 0.08 | 0.15 |

| Slope of output |
| --------------- |
| 0.16            |
| 0.17            |
| 0.16            |

| delta output |
| ------------ |
| -0.13        |
| 0.04         |
| -0.13        |

| Error at hidden layer |        |        |
| --------------------- | ------ | ------ |
| -0.059                | -0.059 | -0.059 |
| 0.015                 | 0.015  | 0.015  |
| -0.015                | -0.015 | -0.015 |

| Delta hidden layer |        |        |
| ------------------ | ------ | ------ |
| -0.008             | -0.007 | -0.008 |
| 0.002              | 0.002  | 0.003  |
| -0.001             | -0.001 | -0.002 |

**Step-10**

<u>Update weight at both output and hidden layer</u>

*wout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)learning_rate
wh =  wh+ matrix_dot_product(X.Transpose,d_hiddenlayer)learning_rate*

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E     |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ----- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.43 | 0.5  | 0.79   | 0    | -0.79 |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.41 |      | 0.79   | 1    | 0.21  |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.10 |      | 0.80   | 0    | -0.80 |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |       |

| Slope of hidden layer |      |      |
| --------------------- | ---- | ---- |
| 0.14                  | 0.12 | 0.14 |
| 0.15                  | 0.13 | 0.20 |
| 0.07                  | 0.08 | 0.15 |

| Slope of output |
| --------------- |
| 0.16            |
| 0.17            |
| 0.16            |

| delta output |
| ------------ |
| -0.13        |
| 0.04         |
| -0.13        |

| Error at hidden layer |        |        |
| --------------------- | ------ | ------ |
| -0.059                | -0.059 | -0.059 |
| 0.015                 | 0.015  | 0.015  |
| -0.015                | -0.015 | -0.015 |

| Delta hidden layer |        |        |
| ------------------ | ------ | ------ |
| -0.008             | -0.007 | -0.008 |
| 0.002              | 0.002  | 0.003  |
| -0.001             | -0.001 | -0.002 |

| Learning Rate |
| ------------- |
| 0.1           |

**Step-11**

<u>Update biases at both output and hidden layer</u>

*bh = bh + sum(d_hidden_layer, axis=0) * learning_rate*
*bout = bout + sum(d_output, axis=0)*learning_rate*

| x    |      |      |      | wh   |      |      | bh   |      |      | hidden_input_layer |      |      | hidden_activation_layer |      |      | wout | bout | output | y    | E     |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ----------------------- | ---- | ---- | ---- | ---- | ------ | ---- | ----- |
| 1    | 0    | 1    | 1    | 0.94 | 0.24 | 0.18 | 0.26 | 0.79 | 0.38 | 1.56               | 1.82 | 1.63 | 0.83                    | 0.86 | 0.84 | 0.43 | 0.48 | 0.79   | 0    | -0.79 |
| 1    | 0    | 0    | 1    | 0.95 | 0.73 | 0.55 |      |      |      | 1.54               | 1.67 | 0.98 | 0.82                    | 0.84 | 0.73 | 0.41 |      | 0.79   | 1    | 0.21  |
| 1    | 1    | 0    | 1    | 0.02 | 0.15 | 0.65 |      |      |      | 2.49               | 2.40 | 1.53 | 0.92                    | 0.92 | 0.82 | 0.10 |      | 0.80   | 0    | -0.80 |
|      |      |      |      | 0.34 | 0.65 | 0.42 |      |      |      |                    |      |      |                         |      |      |      |      |        |      |       |

| Slope of hidden layer |      |      |
| --------------------- | ---- | ---- |
| 0.14                  | 0.12 | 0.14 |
| 0.15                  | 0.13 | 0.20 |
| 0.07                  | 0.08 | 0.15 |

| Slope of output |
| --------------- |
| 0.16            |
| 0.17            |
| 0.16            |

| delta output |
| ------------ |
| -0.13        |
| 0.04         |
| -0.13        |

| Error at hidden layer |        |        |
| --------------------- | ------ | ------ |
| -0.059                | -0.059 | -0.059 |
| 0.015                 | 0.015  | 0.015  |
| -0.015                | -0.015 | -0.015 |

| Delta hidden layer |        |        |
| ------------------ | ------ | ------ |
| -0.008             | -0.007 | -0.008 |
| 0.002              | 0.002  | 0.003  |
| -0.001             | -0.001 | -0.002 |

| Learning Rate |
| ------------- |
| 0.1           |