

Session 3 Notes:

1. Summary:

- Discussion was on Different types of Convolution

2dconv,3dconv,1x1(pointwise),Dilated,Depth Wise Convolution,Transposed Convolution,Spatial Convolution

Point Wise Convolution is same as 1x1 Convolution

- Different ways of pooling

Maxpooling,average pooling,Spacial pyramidical Pooling

- DropOut
- Architectural Basics



**Notes**

[Distill Link](https://distill.pub/2016/deconv-checkerboard/)

**Checkboard issue:**

Can be caused due to multiple reasons:

1.Convolution-When convolving over image/channel we are going through over regions which are counted multiple times,those regions get high values causing checkboard issue.

2.Deconvolution or Transposed Convolution

 The uneven overlaps on the two axes multiply together, creating a characteristic checkerboard-like pattern of varying magnitudes.



Solution:

1.Use padding in small image so that it convolves on the bigger region and hence less overlap.

But it is ineffective as padding as adding 0s and no information

2.**Dilated Convolution**

Reason to use:

1.Less computation hence can be used in realtime or smartphone applications.

A 3x3 kernel with a dilation rate of 2 will have the same view field of a 5x5 kernel. This will increase our field of perception but not increase our computational cost.

2.Increasing the field of view has some the added advantage of increasing the receptive field. This helps the filter capture more contextual information.Moreover, by detecting details in higher resolutions, we can detect finer details in the images.

3.In image segmentation tasks, a dilated convolution is used to keep the input and output images the same size.

However, one disadvantage could be that since we are using a single pixel value to represent an area of pixels, the dilated convolution could be prone to the same spatial loss error that the pooling layers are prone to.

Idea is to spread the 2x2 image into a bigger image by adding separation in the image

where d is the separation between two pixels.

Image size = 
$$
(i+2p-k-(k-1)(d-1))/s+1
$$
![1538848512059](C:\Users\u6yuv\AppData\Roaming\Typora\typora-user-images\1538848512059.png)

![1538849271668](C:\Users\u6yuv\AppData\Roaming\Typora\typora-user-images\1538849271668.png)





The first image shows a normal conv2D filter. Here, the number of parameters is equal to the size of the receptive field. The red dots shows the pixel values that are used for calculating the convolution.

In the second image, we add a dilation rate of 2, which increases the receptive field to 7x7. This means that by increasing the dilation rate, we can increase the receptive field exponentially by linear change of parameters.

The receptive field can be further increased by increasing the dilation rate more as like the image in the far right. By increasing the receptive field, we can integrate more knowledge of the context with less cost.

But the problem again here is how can we fill the numbers in between

Different Approach:

1.knn 

2.

3.

Better Approach

- Shuffle Pixel(jeremy video 14)





**Depth Wise Convolution**

Say you have [15x15x16] as our feature input.We need to change it to [13x13,32]

Steps to do depth wise convolution

1.Separate the layer in different channels -[15x15x1],16

2.Do convolution separately in each channel with filter [3x3x1],16  Output=[13x13x1],16

3.Concatenate all the 16 channels together -[13x13,16]

4.Use 1x1 to increase channel [1x1x16],32   Output=[13x13x32]

Total Parmeters: 3x3x16 + 16x32x1x1 =656 parameters

However in normal convolution

3x3x16x32=4608 parameters





3.**SEPARABLE CONVOLUTION**
The problem with the traditional convolution layer is that it has too many parameters. A 3x3 convolution layer has 9 parameters and this number increases by a power of 2 every time we increase our kernel size.

The challenge with too many parameters is that not only does it take a long time to learn these parameters while training, but it also takes equally long to make predictions while performing inference.

The dilated convolution decreases the number of parameters and makes it useful for performing image segmentation. The separable convolution reduces the computational cost and the number of parameters further so that it can be used in mobile and IoT devices.

**How the Separable Convolution works:** A convolution is a vector multiplication that gives us a certain result. We can get the same result, by multiplying with two smaller vectors. This is better understood with an example.

A normal 3x3 convolution has 9 parameters. The same 3x3 convolution can be computed as one 1x3 convolution followed by a 3x1 convolution. By applying them one after another, we can get the same effect as a 3x3 convolution. But we have now reduced the number of parameters to 6, thereby reducing our computational cost.

Different filters are merged or added in separable convolutions in a different way. To get the same effect as a 3x3 convolution, we cannot merge the separate 1x3 and 3x1 convolutional feature maps. Instead, we merge them at the end of the 3x1 convolution operation.

**Where is it used:** The separable convolution has been used immensely in the Xception CNN. The Xception CNN was designed by F Chollet who is also the author of the Keras Deep Learning Library. Xception was designed keeping in mind that most other CNN architectures before that were too large to be used in real time in mobile or IoT devices.

By reducing the number of parameters, the Xception model became a CNN that could be deployed in the real world. The graph below shows a comparison of the accuracy and the number of parameters in the different CNN models.

Xception and Inception, owing to their smaller sizes have found extensive use in embedded systems like in Raspberry Pi’s and Arduino’s.

[![img](https://www.saama.com/wp-content/uploads/2017/12/10.jpg)](https://towardsdatascience.com/neural-network-architectures-156e5bad51ba)

How to do:

First use fx1 where f=width,1=height filter to traverse over all the columns (horizontally)

Then use 1xf to traverse vertically







**DECONVOLUTION**
Before I explain what deconvolution is, it is important to understand what a convolution layer is doing intuitively.

A convolution converts all the pixels in its receptive field into a single value. When we apply a convolution to an image, we are not only decreasing the image size (downsampling), but we are also bringing all the information in the field together into a single pixel.

The final output of the convolutional layer is a vector. This vector can be called an image embedding as it contains all the information needed to understand what is present in that image.

This turns out to be important when we are trying to predict what is in an image as it concentrates the important features. At the same time though, it reduces the resolution of images, which is great for predicting classes, but not for image segmentation or generation. In these cases, we need to go from a lower resolution image to a higher one. This is where a deconvolution layer comes in.

The name deconvolution is not apt, because we aren’t actually performing a deconvolution. A convolution is a multiplication in a Fourier space and a deconvolution on a convoluted image (in image processing) gives us the original image.

Deconvolution (in its image processing essence) cannot be done in machine learning, as a Gaussian blurring of an image, in case of a convolutional layer, is an invertible process. What this means is that the deconvolution operation is a black box of sorts, and no one has quite figured out how to get the original pixel values back from a convoluted image. In machine learning, however, a deconvolution is another convolution layer, that spaces out the pixels and performs an up sampling.

A convolution integrates the information spread over a large area into a single pixel; whereas a deconvolution spreads that information out over a large area.

Deconvolutions have another name to prevent confusion: Transposed Convolutions.

**How the deconvolution works:** It is not possible to interpolate a single pixel value into many values.

[![img](https://www.saama.com/wp-content/uploads/2017/12/1_BMngs93_rm2_BpJFH2mS0Q.gif)](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)

![img](https://www.saama.com/wp-content/uploads/2017/12/1_Lpn4nag_KRMfGkx1k6bV-g.gif)

Convolution Deconvolution

The quick and dirty solution is to do some fancy padding to the image and then apply a convolution operation. This operation will obviously not result in the same pixel values of the original image. In fact, it might result in some very weird or a lot of zero values. However, it does bring back the image to its original spatial dimensions, which is what we were trying to do initially.

**Where to use the Deconvolution layer:** As I mentioned before, deconvolutions are used in image segmentation tasks.

When we apply a convolution, we reduce the spatial dimensions of the image. Deconvolutions are done after the convolution layer to keep the size of the output image same as before.



**Points**

- CNN for Image Compression
- Separable Convolution-Nvidea has Supported accelarators
- Average pooling- answer to all kind of resolution
- Image Segmentation,SuperResolution,Colorization
- Maxpooling-Reduces layers,increases receptive field





**<u>Differences</u>**

1. Dilated vs transposed convolution / fractionally strided convolution / deconvolution

They both do the task of up-sampling a layer.but there are subtle differences :

**Transposed Convolution**

It is the Input that needs to be dilated to express deconvolution.

A deconvolution corresponds to the backward pass of a regular strided convolution w.r.t. the input, i.e. it is a convolution with a stride in the *output* space, rather than a stride in the *input* space.

This is equal to the function we used for convolution in the back-propagation.

*Simply in backprop we distribute gradients from one neuron in the output feature map to all the elements in the receptive fields, then we also sum up gradients for where they coincided with same receptive elements*

So the basic idea is deconvolution works in the output space. Not input pixels. It will try to create more broader spacial dimensions in the output map. This is used in **Fully Convolutional Neural Nets for Semantic Segmentation**.

**So more of Deconvolution is a learnable up-sampling layer.**

**It's tries to learn how to up-sample while combining with the final loss**

This is the best explanation I found for deconvolution. Lecture 13 in cs231,





**Dilation**

It is the filter that is dilated in the case of "dilated convolution".

A dilated convolution is a convolution with a stride in the *filter* space. Both its input and output strides are 1.

This is more like exploring the **input data points in a wide manner**. Or increasing the receptive field of the convolution operation.

It help to capture more and more global context from input pixels without increasing the size of the parameters. This can also help to increase the spacial size of the output. But the main thing here is this increases the receptive field size exponentially with the number of layers.

In short, dilated convolution is a simple but effective idea and you might consider it in two cases;

1. Detection of fine-details by processing inputs in higher resolutions.
2. Broader view of the input to capture more contextual information.
3. Faster run-time with less parameters

For example, semantic segmentation with one label per pixel; image super-resolution, denoising, demosaicing, bottom-up saliency, keypoint detection, etc.

To make this more concrete, let's take a very simple example: 
Say you have a 9x9 image, **x** with no padding. If you take a standard 3x3 kernel, with stride 2, the first subset of concern from the input will be **x**[0:2, 0:2], and all nine points within these bounds will be considered by the kernel. You would then sweep over **x**[0:2, 2:4] and so on.

Clearly, the output will have smaller facial dimensions, specifically 4x4. Thus, the next layer's neurons have receptive fields in the exact size of these kernels passes. But if you need or desire neurons with more global spatial knowledge (e.g if an important feature is only definable in regions larger than this) then you will need to convolve this layer a second time to create a third layer in which the effective receptive field is some union of the previous layers r.f..

But if you don't want to add more layers and/or you feel that the information being passed on is overly redundant (i.e. your 3x3 receptive fields in the second layer only actually carry "2x2" amount of distinct information), you can use a dilated filter. Let's be extreme about this for clarity and say we'll use a 9x9 3-dialated filter. Now, our filter will "encircle" the entire input, so we won't have to slide it at all. We will still however, only be taking 3x3=9 data points from the input, **x**, typically:

**x**[0,0] U **x**[0,4] U **x**[0,8] U **x**[4,0] U **x**[4,4] U **x**[4,8] U **x**[8,0] U **x**[8,4] U **x**[8,8]

Now, the neuron in our next layer (we'll only have one) will have data "representing" a much larger portion of our image, and again, if the image's data is highly redundant for adjacent data, we may well have preserved the same information and learned an equivalent transformation, but with fewer layers and fewer parameters. I think within the confines of this description it's clear that while definable as resampling, we are here *downsampling* for each kernel.



Different Normalization Technique

http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/

Batch Norm:

- Stable if the batch size is large
- Robust (in train) to the scale & shift of input data
- Robust to the scale of weight vector
- Scale of update decreases while training

- Not good for online learning
- Not good for RNN, LSTM
- Different calculation between train and test

Weight Norm:

- Smaller calculation cost on CNN
- Well-considered about weight initialization
- Implementation is easy
- Robust to the scale of weight vector

- Compared with the others, might be unstable on train?
- High dependence to input data

Layer Norm:

- Effective to small mini batch RNN
- Robust to the scale of input
- Robust to the scale and shift of weight matrix
- Scale of update decreases while training

- Might be not good at CNN?
  (Batch Norm is better in some cases)