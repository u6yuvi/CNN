Discrete Convolution

Motivation:

Images, sound clips and many other similar kinds of data have an intrinsic structure. More formally, they share these important properties:

1. They are stored as multi-dimensional arrays.

2. They feature one or more axes for which ordering matters (e.g., width and

   height axes for an image, time axis for a sound clip).

3. One axis, called the channel axis, is used to access different views of the

   data (e.g., the red, green and blue channels of a color image, or the left

   and right channels of a stereo audio track).

These properties are not exploited when an affine transformation is applied in fact(in FEED Forward Neural network), all the axes are treated in the same way and the topological information is not taken into account.However taking advantage of the implicit structure of the data may prove very handy in solving some tasks, like computer vision and speech recognition, and in these cases it would be best to preserve it. This is where discrete convolutions come into play.

A discrete convolution is a linear transformation that preserves this notion of ordering. It is sparse (only a few input units contribute to a given output unit) and reuses parameters (the same weights are applied to multiple locations in the input).

Building Blocks of Convolutional Neural networks:

Strides-Consider them as a way of doing subsampling.For instance, moving the kernel by hops of two is equivalent to moving the kernel by hops of one but retaining only odd output elements.



Pooling

Pooling operations reduce the size of feature maps by using some function to summarize subregions, such as taking the average or the maximum value.
Pooling works by sliding a window across the input and feeding the content of the window to a pooling function. In some sense, pooling works very much like a discrete convolution, but replaces the linear combination described by the kernel with some other function. Figure 1.5 provides an example for average
pooling, and Figure 1.6 does the same for max pooling.



Convolution arithmetic

- No zero padding,unit strides

  i,k,p=0,s=1

$$
o=(i-k)+1
$$

For eg: 

i=5,k=3,p=0,s=1

o=3  ([(5-3)+1]

![1538836300400](C:\Users\u6yuv\AppData\Roaming\Typora\typora-user-images\1538836300400.png)



- Zero padding unit strides

  padding with p zeros changes the effective input size from i to (i + 2p).

  For any i,k,p and s=1
  $$
  o=(i-k) +2p+1
  $$


For eg:

i=5,k=3,p=2,s=1

o= 7 [(5-3)+2X2+1]

![1538836336615](C:\Users\u6yuv\AppData\Roaming\Typora\typora-user-images\1538836336615.png)

- Half(Same Padding)

  Having the output size same as the input size

  i,k (2n+1),s=1,p=[k/2]=n
  $$
  o=(i-k)+2[k/2]+1
  $$
  Rearranging the terms:
  $$
  o=i+2[k/2]-(k-1)
  $$


For eg:

i=5,o=5

k=3 (2x1+1) i.e. n=1

p=n=1

o=5 (5+2-2)

![1538836365399](C:\Users\u6yuv\AppData\Roaming\Typora\typora-user-images\1538836365399.png)

- Full Padding

  While convolving a kernel generally decreases the output size with respect to the input size, sometimes the opposite is required. This can be achieved with proper zero padding

For any i and k, and for p=k-1 and s=1
$$
o=i+2(k-1)-(k-1)
$$
which is same as:
$$
o=i+(k-1)
$$
![1538836383453](C:\Users\u6yuv\AppData\Roaming\Typora\typora-user-images\1538836383453.png)



- No Zero Padding,Non Unit Strides

For any i,k and s and for p=0
$$
o=[i-k/s]+1
$$

- Zero Padding,non unit strides

For any i,k,p and s:
$$
o=[(i-k+2p)/s]+1
$$


Pooling 

For any i,k and s
$$
o=[(i-k)/s]+1
$$


**Transposed Convolution Arithmetic (Fractionally Strided Convolution)

The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that
is compatible with said convolution.

Steps:

1.Unroll the filter into a matrix 

Say we have a filter of size 3x3.Unrolling it to 4x16 and call it C

2.Unroll the image into a n dimensional vector,in this case of shape [16,1]

So in the forward pass,After convolution,we will get a feature map of [4x1] which can be rolled to [2x2] 

If you compare with the normal way of convolution,we will get the same answer

i=4,k=3,p=0,s=1

o=2

However in the backward pass,the error is propagated by multiplying the loss with C(T)

**Transposed convolution**

Letâ€™s now consider what would be required to go the other way around, i.e. map from a 4-dimensional space to a 16-dimensional space and this process is called transposed convolution

- **No zero padding,unit strides,transposed**

  A convolution described by s = 1, p = 0 and k has an associated transposed convolution described by:

   k' = k, s'= s and p' = k -1 and its output size is
  $$
  o'=i'+(k-1)
  $$


Same as fully padded convolution with unit strides

Zero Padding,unit strides,transposed

Knowing that the transpose of a non-padded convolution is equivalent to convolving a zero padded input, it would be reasonable to suppose that the transpose of a zero padded convolution is equivalent to convolving an input padded with less zeros.

A convolution described by s = 1, k and p has an
associated transposed convolution described by k' = k, s' = s and
p' = k - p -1 and its output size is
$$
o'=i'+(k-1)-2p
$$
